# -*- coding: utf-8 -*-
"""British-Airways

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MadxCRBx0RqGtl1lizn07Ts6wuMCB0in

**## import necessary libraries ##**
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 10
page_size = 100

reviews = []

# for i in range(1, pages + 1):

for i in range(1, pages + 1):
  print(f"Scraping page {i}")

  # Create URL to collect links from paginated data
  url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"

   # Collect HTML data from this page
  response = requests.get(url)

  # Parse content
  content = response.content
  parsed_content = BeautifulSoup(content, 'html.parser')
  for para in parsed_content.find_all("div", {"class": "text_content"}):
      reviews.append(para.get_text())

  print(f"   ---> {len(reviews)} total reviews")

df = pd.DataFrame()
df["reviews"] = reviews
df.head()

import os

os.makedirs("data", exist_ok=True)

df.to_csv("data/British_reviews.csv")

"""We have our dataset for the next process.

**##DataCleaning**
"""

df["reviews"] = df["reviews"].str.split("|", expand=True)[1]
df.head()

import string

def case_and_punctuation_handling(text):
  lower_case_text = text.lower()
  cleaned_text = lower_case_text.translate(str.maketrans("", "", string.punctuation))
  return cleaned_text
df["cleaned_reviews"] = df["reviews"].apply(case_and_punctuation_handling)
df.head()

"""from the above code, we have converted upper case letters to lower case and removed punctuations.

**# tokenization, stopwords removal and POS tagging**
"""

import nltk

nltk.download("punkt")

nltk.download("stopwords")

nltk.download("wordnet")

nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords, wordnet

# POS tagging dictionary.
pos_dict = {"J":wordnet.ADJ, "V":wordnet.VERB, "N":wordnet.NOUN, "R":wordnet.ADV}

def handle_tokenization_stopwords_postags(text):
    # Tokenization.
    tokenized_text = word_tokenize(text)
    # POS-tagging.
    tags = pos_tag(tokenized_text)
    new_text_list = []
    for word, tag in tags:
        # Stopword removal.
        if word not in stopwords.words("english"):
          new_text_list.append(tuple([word, pos_dict.get(tag[0])]))
    return new_text_list

df["POS_tagged_reviews"] = df["cleaned_reviews"].apply(handle_tokenization_stopwords_postags)
df.head()

"""**##Lemmatization**"""

# Obtaining stem-words with Lemmatization

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

def text_lemmatization(pos_data):
    lemma_rev = " "
    for word, pos in pos_data:
     if not pos:
        lemma = word
        lemma_rev = lemma_rev + " " + lemma
     else:
        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)
        lemma_rev = lemma_rev + " " + lemma
    return lemma_rev

df["lemmatized_reviews"] = df["POS_tagged_reviews"].apply(text_lemmatization)
df.head()

"""**##Sentiment_Analysis**"""

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def vader_analysis(sentiment_text):
    score = analyzer.polarity_scores(sentiment_text)
    return score["compound"]

df["VADER_compound_score"] = df["lemmatized_reviews"].apply(vader_analysis)

# function to analyse
def sentiment_classifier(compound_score):
    if compound_score >= 0.5:
        return "Positive"
    elif compound_score < 0 :
        return "Negative"
    else:
        return "Neutral"

df["Sentiment"] = df["VADER_compound_score"].apply(sentiment_classifier)
df.head()

sentiment_counts = df["Sentiment"].value_counts()
sentiment_counts

df.to_csv("data/BA_reviews_analysed.csv")

"""**##Data Vizualization: Pie-Chart**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Declaring data.
data = sentiment_counts.values

# Define Seaborn color palette to use.
palette_color = sns.color_palette("dark") # Use a dark color palette instead

# Plotting data on chart.
plt.pie(data, labels=sentiment_counts.index, colors=palette_color, explode = (0, 0, 0.15), autopct="%.1f%%")

# Displaying chart.
plt.show()

"""**##Data Vizualization: Word Cloud**"""

from wordcloud import WordCloud

def vizualize_wordcloud(data):
    wordcloud = WordCloud(background_color="black", max_words=200, max_font_size=35, scale=4, relative_scaling=0.9, random_state=7)
    wordcloud = wordcloud.generate(str(data))

    fig = plt.figure(1, figsize=(10, 10))
    plt.axis("off")

    plt.imshow(wordcloud)
    plt.show()

vizualize_wordcloud(df["lemmatized_reviews"])

"""**##Predicting customer buying behaviour**

#Exploratory Data Analysis
"""

# Importing all packages.

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif as mic
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings("ignore")

# Importing dataset.

cust = pd.read_csv("customer_booking.csv", encoding='latin-1') # Try reading with 'latin-1' encoding
cust.head()

cust.tail()

"""The .head() method allows us to view the first 5 rows in the dataset"""

cust.shape

"""checking the statistical of numerical columns"""

# Checking columns datatypes.
cust.info()

cust.describe()

# Categorical columns.
cat_cols = cust.select_dtypes("object")

# Checking the unique values in Categorical columns.
for col in cat_cols:
    print("\nUnique values for column '{}':".format(col))
    print(cust[col].unique(), "\nUnique count: {}".format(cust[col].nunique()))

# Label Encoding the categorical variables.
label_encode = LabelEncoder()

for col in cat_cols:
    cust[col] = label_encode.fit_transform(cust[col])
    print("\nUnique values for column '{}':".format(col))
    print(cust[col].unique(), "\nUnique count: {}".format(cust[col].nunique()))

# Checking the datatypes of converted columns.
cust.info()

cust.describe()

# Converting the dataset into features and label.
X = cust.drop("booking_complete", axis=1)
y = cust["booking_complete"]

# Calculating the Mutual Information Scores.

fi_scores = mic(X, y)
fi_df = pd.DataFrame({"Columns": X.columns, "Feature_Importance_Score": fi_scores})
fi_df = fi_df.sort_values(by="Feature_Importance_Score", ascending=False)
fi_df

"""**##Data Vizualization: Feature Importance Barplot**"""

plt.figure(figsize=(8, 5))
sns.barplot(x="Feature_Importance_Score", y="Columns", data=fi_df, palette="dark")
plt.title("Feature Importance using Mutual Information Scores")
plt.xlabel("")
plt.ylabel("")
plt.show()

"""**##Model Training and Cross-Validation**"""

# Function to split the data into training and validation set.
def train_val_split(X, y):
    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=7)
    return train_X, val_X, train_y, val_y

# Function to select the top-n or all the features from data.
def selecting_top_n_or_all_features(n=5):
    if str(n).lower() == "all":
        X = df[list(fi_df.Columns)]
    else:
        X = df[list(fi_df.Columns[:n])]
    # One-Hot-Encoding the variables which were Categorical variables prior to Label-Encoding.
    for col in X.select_dtypes("int32"):
        X = pd.get_dummies(X, columns=[col])
    return X

# Function to fit the data on RandomForestClassifier and product training and validation scores.
def fit_rfc(top_n):
    X = selecting_top_n_or_all_features(top_n)
    train_X, val_X, train_y, val_y = train_val_split(X, y)

    # Normalizing the Dataset.
    scaler = MinMaxScaler()
    train_X = scaler.fit_transform(train_X)
    val_X = scaler.transform(val_X)

    model = RandomForestClassifier(random_state=7)
    model.fit(train_X, train_y)

    train_y_pred = model.predict(train_X)
    val_y_pred = model.predict(val_X)

    print("Training Accuracy Score:", accuracy_score(train_y, train_y_pred))
    print("Validation Accuracy Score:", accuracy_score(val_y, val_y_pred))

print("\n===== Model Evaluation for Top-6 features =====\n")
fit_rfc(6)

# Data for the bar plot
data = {"Accuracy": [0.85, 0.72],
        "Set": ["Training", "Validation"]}
# Create the bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x="Set", y="Accuracy", data=data, palette="dark")
plt.title("Model Accuracy on Training and Validation Sets (Top-6 Features)")
plt.ylim(0, 1)  # Set y-axis limits for better comparison
plt.show()

print("\n===== Model Evaluation for All features =====\n")
fit_rfc("all")

# Data for the bar plot
data = {"Accuracy": [0.999825, 0.8539],
        "Set": ["Training", "Validation"]}
# Create the bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x="Set", y="Accuracy", data=data, palette="dark")
plt.title("Model Accuracy on Training and Validation Sets (All Features)")
plt.ylim(0, 1)  # Set y-axis limits for better comparison
plt.show()